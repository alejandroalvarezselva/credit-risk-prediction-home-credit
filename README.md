<p align="center">
  <img src="image%20Home%20Credit.png" alt="Home Credit Banner" width="100%">
</p>


# üìå Credit Risk Prediction ‚Äì Home Credit

Proyecto de **predicci√≥n de riesgo de cr√©dito** utilizando datos reales del caso Home Credit Default Risk.  
El **objetivo es estimar la probabilidad de impago** de cada cliente mediante un modelo de clasificaci√≥n robusto, interpretable y reproducible.

El dataset original es complejo y multitabla, con informaci√≥n financiera, del bur√≥ de cr√©dito y del historial de pagos.  
Este proyecto reproduce un flujo real de scoring bancario: desde la integraci√≥n de datos y el EDA hasta la interpretaci√≥n con SHAP y la generaci√≥n de predicciones finales.

Este repositorio forma parte de mi formaci√≥n como **Data Analyst / Data Scientist**, y demuestra un flujo profesional y completo de modelado de riesgo.

---

## üìÅ Estructura del repositorio

Este repositorio contiene el notebook principal del proyecto, junto con los artefactos reproducibles generados en el proceso de modelado:

```text
credit-risk-prediction-home-credit/
‚îÇ
‚îú‚îÄ‚îÄ credit-risk-prediction-home-credit.ipynb      # Notebook principal (EDA + ML + SHAP + scoring)
‚îÇ
‚îú‚îÄ‚îÄ artifacts/                                    # Artefactos reutilizables
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing_pipeline_v3.joblib          # Pipeline de preprocesamiento (fitted)
‚îÇ   ‚îú‚îÄ‚îÄ feature_names_v3.npy                      # Nombres de las 357 features finales
‚îÇ   ‚îî‚îÄ‚îÄ lgbm_optimized_v1.joblib                  # Modelo LightGBM optimizado
‚îÇ
‚îú‚îÄ‚îÄ submissions/
‚îÇ   ‚îî‚îÄ‚îÄ submission_lightgbm_20251210_1343.csv     # Predicciones finales para test
‚îÇ
‚îî‚îÄ‚îÄ README.md
```


## üéØ Objetivo del proyecto

El proyecto aborda el problema desde dos perspectivas complementarias: **negocio** (decisiones de riesgo) y **t√©cnica** (construcci√≥n de un modelo fiable y reproducible).

### **Objetivo de negocio (Riesgo de Cr√©dito)**

- Estimar la **probabilidad de impago** de cada solicitante.
- Reducir p√©rdidas evitando conceder pr√©stamos a clientes de alto riesgo.
- Mantener la aprobaci√≥n de clientes solventes para no afectar la rentabilidad.
- Utilizar un modelo **interpretativo y justificable**, adecuado para auditor√≠as y entornos regulados.

### **Objetivo t√©cnico**

- Construir un **pipeline reproducible** de preparaci√≥n, modelado y scoring.
- Evaluar diferentes algoritmos y seleccionar el modelo √≥ptimo.
- Emplear **m√©tricas espec√≠ficas del sector financiero** (AUC, KS) para garantizar validez en riesgo de cr√©dito.
- Incorporar **interpretabilidad con SHAP** para justificar predicciones individuales y globales.
- Generar un archivo final de scoring listo para integrarse en sistemas de decisi√≥n.


## üìä Datos

El proyecto utiliza m√∫ltiples tablas del dataset **Home Credit Default Risk**, que se integran mediante un proceso de *feature engineering* para generar un dataset enriquecido adecuado para el modelado.

Tras esta integraci√≥n se obtiene:

- **train:** 307 511 filas, 524 columnas  
- **test:** 48 744 filas, 523 columnas  

La diferencia de columnas entre train y test se debe a categor√≠as presentes solo en el entrenamiento, lo que refuerza la necesidad de un preprocesamiento robusto con `OneHotEncoder(handle_unknown="ignore")`.

El dataset enriquecido se guarda como un *checkpoint* intermedio, lo que permite:

- acelerar el ciclo de experimentaci√≥n,  
- garantizar consistencia en los an√°lisis,  
- evitar repetir procesos costosos de ingenier√≠a de caracter√≠sticas.

Este checkpoint es la base sobre la que se realiza el EDA y se entrena el modelo final.


## üîç EDA y selecci√≥n de variables

El an√°lisis exploratorio permiti√≥ evaluar la calidad del dataset enriquecido y detectar aspectos cr√≠ticos para la construcci√≥n de un modelo de riesgo de cr√©dito. Entre los principales hallazgos se encuentran:

- **Distribuciones sesgadas:** variables financieras como ingresos, importes o duraci√≥n del cr√©dito mostraron fuerte asimetr√≠a y presencia de outliers, lo que motiv√≥ el uso de imputaci√≥n robusta (mediana) y controles espec√≠ficos.
- **Valores faltantes:** varias columnas presentaban porcentajes elevados de nulos; se eliminaron aquellas con ausencia excesiva y se defini√≥ una estrategia de imputaci√≥n diferenciada para variables num√©ricas y categ√≥ricas.
- **Outliers relevantes:** se identificaron valores extremos que pod√≠an distorsionar el entrenamiento, especialmente en ingresos y montos, justificando su tratamiento.
- **Correlaciones altas:** se detectaron grupos de variables muy correlacionadas (especialmente en informaci√≥n de bur√≥), lo que llev√≥ a reducir redundancia para mejorar la estabilidad del modelo.
- **Cardinalidad excesiva en categ√≥ricas:** algunas variables ten√≠an demasiados niveles, lo que habr√≠a generado cientos de columnas tras la codificaci√≥n. Se filtraron para evitar sobrecarga y ruido.
- **Desbalance de clases:** TARGET=1 era significativamente menos frecuente, reforzando la necesidad de usar m√©tricas adecuadas como AUC y KS.

Tras este proceso se defini√≥ un conjunto final de **357 variables limpias, estables y modelables**, que sirvi√≥ de base para el pipeline de preprocesamiento y el entrenamiento del modelo.


## ‚öôÔ∏è Preprocesamiento (pipeline v3)

El objetivo del preprocesamiento es transformar el dataset enriquecido en un conjunto **limpio, consistente y totalmente num√©rico**, garantizando que tanto *train* como *test* reciben exactamente las mismas transformaciones.

El dise√±o del pipeline v3 se basa directamente en los hallazgos del EDA.

---

### **1. Variables num√©ricas**
- **Imputaci√≥n con mediana:** seleccionada por su robustez frente a outliers, frecuentes en variables financieras como ingresos o importes.  
- **Sin escalado:** modelos basados en √°rboles (como LightGBM) no requieren normalizaci√≥n ni estandarizaci√≥n, lo que evita transformaciones innecesarias.

---

### **2. Variables categ√≥ricas**
- **Imputaci√≥n con moda:** adecuada para categor√≠as con distribuci√≥n muy concentrada, evitando crear niveles artificiales.  
- **Codificaci√≥n con OneHotEncoder (`handle_unknown="ignore"`):**  
  - evita errores cuando aparecen categor√≠as nuevas en el conjunto de test,  
  - mantiene estabilidad en scoring,  
  - es la t√©cnica m√°s adecuada cuando las categ√≥ricas no tienen cardinalidad excesiva (las de alta cardinalidad fueron filtradas en el EDA).

---

### **3. Salida del pipeline**
Tras la imputaci√≥n y codificaci√≥n, el dataset final se transforma en una matriz de **357 features estables**, lista para el entrenamiento del modelo.

El pipeline se guarda como artefacto reproducible en:

artifacts/preprocessing_pipeline_v3.joblib


Este dise√±o asegura que **no existe fuga de informaci√≥n** y que el proceso de scoring es **100% consistente y replicable** con respecto al entrenamiento.



## ü§ñ Modelado

Se evaluaron varios algoritmos de clasificaci√≥n con el objetivo de identificar un modelo capaz de discriminar de forma robusta entre clientes con alta y baja probabilidad de impago.  
La comparaci√≥n se realiz√≥ utilizando un conjunto de validaci√≥n estratificado y m√©tricas est√°ndar del sector financiero (AUC y KS).

### **Modelos comparados**
- Logistic Regression  
- CatBoost  
- XGBoost  
- **LightGBM (modelo final seleccionado)**  

---

### **Resultados en validaci√≥n**

| Modelo               | AUC      | KS      |
|---------------------|----------|---------|
| **LightGBM**        | **0.7783** | **0.4245** |
| XGBoost             | 0.7771   | 0.4246 |
| CatBoost            | 0.7733   | 0.4175 |
| Logistic Regression | 0.6436   | 0.2135 |

---

### üéØ **Conclusiones del modelado**

El an√°lisis comparativo muestra que **LightGBM ofrece el mejor equilibrio entre rendimiento, estabilidad y velocidad**, con m√©tricas alineadas con los est√°ndares de modelos productivos en scoring de cr√©dito:

- **AUC ‚âà 0.78** ‚Üí buena capacidad discriminante en entornos con fuerte desbalance de clases.  
- **KS ‚âà 0.42** ‚Üí nivel propio de modelos s√≥lidos en banca minorista.

Adem√°s, LightGBM resulta especialmente adecuado para este problema debido a:

- su **robustez frente a valores faltantes y outliers**, frecuentes en datos financieros reales,  
- su capacidad para manejar **miles de variables heterog√©neas**,  
- su excelente relaci√≥n **velocidad / rendimiento** en comparaci√≥n con otros modelos de boosting,  
- la facilidad para integrarse con t√©cnicas de interpretabilidad como **SHAP**.

Por estos motivos, **LightGBM fue seleccionado como el modelo final** para el despliegue del sistema de scoring.


## üìà Interpretaci√≥n de m√©tricas (contexto de riesgo de cr√©dito)

La evaluaci√≥n del modelo se centra en dos m√©tricas fundamentales en scoring financiero: **AUC** y **KS**.  
Ambas permiten medir la capacidad del modelo para separar buenos y malos pagadores en entornos con fuerte desbalance de clases.

---

### **AUC (Area Under the ROC Curve)**  
Mide la capacidad del modelo para distinguir correctamente entre clientes solventes e insolventes.  

Un AUC de **‚âà 0.78** indica un modelo:

- **s√≥lido y estable**,  
- adecuado para problemas reales de riesgo de cr√©dito,  
- robusto ante desbalance de clases (TARGET = 1 es minoritario).

---

### **KS (Kolmogorov‚ÄìSmirnov)**  
Es la m√©trica m√°s utilizada en banca, ya que refleja la **separaci√≥n real entre las distribuciones de buenos y malos clientes**.

Referencias del sector:
- 0.20 ‚Üí aceptable  
- 0.30 ‚Üí bueno  
- **0.40+ ‚Üí muy bueno**

El modelo final obtiene **KS ‚âà 0.42**, lo que implica:

- **excelente poder discriminante**,  
- alta capacidad para reducir p√©rdidas por impago,  
- comportamiento consistente y reproducible en validaci√≥n.

---

El modelo final **LightGBM** se almacena como artefacto reproducible en:

`artifacts/lgbm_optimized_v1.joblib`


## üß™ Optimizaci√≥n del modelo (LightGBM)

Para mejorar el rendimiento del modelo base, se realiz√≥ una b√∫squeda de hiperpar√°metros mediante **RandomizedSearchCV**, una t√©cnica eficiente para explorar espacios amplios sin el elevado coste computacional de Grid Search.

### **Hiperpar√°metros optimizados**

Los par√°metros ajustados fueron:

- `n_estimators`  
- `learning_rate`  
- `num_leaves`  
- `max_depth`  
- `min_child_samples`  
- `subsample`  
- `colsample_bytree`  
- `reg_alpha`  
- `reg_lambda`  

Estos par√°metros son especialmente cr√≠ticos en riesgo de cr√©dito porque controlan:

- la complejidad del modelo,  
- el riesgo de sobreajuste,  
- la capacidad de generalizaci√≥n.

### **Mejores resultados obtenidos**

- **AUC Train:** 0.8255  
- **AUC Valid:** 0.7798  
- **KS Valid:** 0.4259  

La diferencia entre train y valid es **moderada**, lo que indica:

- buena capacidad de generalizaci√≥n,  
- ausencia de sobreajuste significativo,  
- estabilidad del modelo para scoring real.

El modelo final se guarda como artefacto reproducible en:
artifacts/lgbm_optimized_v1.joblib


## üß† Interpretabilidad con SHAP

En riesgo de cr√©dito no basta con obtener buenas m√©tricas: es imprescindible **explicar** por qu√© un modelo asigna a un cliente una probabilidad alta o baja de impago.  
Por este motivo se aplic√≥ **SHAP** para analizar la contribuci√≥n de cada variable al modelo LightGBM.

### üîç ¬øQu√© aporta SHAP en este proyecto?

Con SHAP pudimos:

- identificar qu√© variables **incrementan o reducen** la probabilidad de impago,
- comprender patrones globales del riesgo en el portafolio,
- justificar decisiones de cr√©dito ante negocio y auditor√≠a,
- detectar comportamientos no intuitivos o relaciones no lineales capturadas por LightGBM.

### üìå Principales hallazgos (extra√≠dos del an√°lisis SHAP)

Los resultados de SHAP confirmaron patrones esperados en modelos de scoring:

- Los **retrasos previos en pagos** y variables relacionadas con morosidad fueron las que **m√°s aumentaron la probabilidad de impago**.
- La informaci√≥n del **bur√≥ crediticio** (cr√©ditos activos, atrasos hist√≥ricos, montos pendientes) mostr√≥ una influencia significativa.
- El **ratio entre pagos e ingresos** y otros indicadores de capacidad de pago tuvieron un impacto importante.
- Una mayor **intensidad del historial crediticio** (n√∫mero y antig√ºedad de productos) tendi√≥ a reducir el riesgo al aportar estabilidad.

Estas conclusiones est√°n alineadas con lo que se observa en modelos reales de bancos y entidades financieras.

### üéØ Valor para el negocio

La interpretabilidad aportada por SHAP garantiza:

- **transparencia** del modelo,
- **trazabilidad** de cada decisi√≥n,
- cumplimiento de requisitos regulatorios,
- confianza para su potencial uso en un sistema de aprobaci√≥n de cr√©dito.


## üì§ Scoring final

La fase de scoring aplica el **pipeline de preprocesamiento** y el **modelo final optimizado** para generar la probabilidad de impago (PD) de nuevos clientes utilizando el conjunto de test.

### üîÑ Proceso de scoring
1. Se carga el dataset enriquecido de test.
2. Se aplica el pipeline `preprocessing_pipeline_v3.joblib` para asegurar exactamente las mismas transformaciones que en entrenamiento.
3. El modelo `lgbm_optimized_v1.joblib` genera la probabilidad estimada de impago para cada cliente.
4. Se construye el archivo final de predicciones:
   submissions/submission_lightgbm_20251210_1343.csv


### üìÑ Contenido del archivo de salida
- `SK_ID_CURR` ‚Äî identificador √∫nico del cliente.  
- `TARGET` ‚Äî **probabilidad estimada de impago (PD)** generada por el modelo.

### üßæ Utilidad del scoring
Este fichero constituye la salida est√°ndar de un sistema de riesgo de cr√©dito y puede integrarse directamente en:
- motores de decisi√≥n autom√°ticos,
- validaciones internas,
- simulaciones de pol√≠ticas de cr√©dito,
- an√°lisis posteriores de negocio o regulador.

El proceso garantiza **consistencia, replicabilidad y ausencia de fugas de informaci√≥n**, ya que train y test pasan por el mismo pipeline.


## üîÅ Reproducibilidad

El proyecto est√° dise√±ado para ser **totalmente reproducible**, de forma que cualquier usuario pueda replicar el entrenamiento, el preprocesamiento y el scoring sin modificar el c√≥digo original.

Los elementos clave que garantizan esta reproducibilidad son:

- Todo el flujo est√° concentrado en el notebook `credit-risk-prediction-home-credit.ipynb`.
- El preprocesamiento y el modelo se encapsulan en artefactos (`joblib`, `npy`) dentro de la carpeta `artifacts/`.
- El mismo pipeline que se usa para entrenar se usa para hacer scoring, evitando inconsistencias entre train y test.

Para reproducir el proyecto, los pasos generales son:

1. Clonar el repositorio:
```bash
git clone https://github.com/alejandroalvarezselva/credit-risk-prediction-home-credit.git
```
2. Descargar los datos originales desde Kaggle (Home Credit Default Risk).
3. Ejecutar el notebook en Google Colab o entorno local, ajustando las rutas seg√∫n sea necesario.

Con estos pasos se puede replicar todo el flujo: EDA, preprocesamiento, entrenamiento, evaluaci√≥n, interpretabilidad y scoring final.


## üìù Conclusiones

El proyecto permiti√≥ construir un sistema completo y reproducible de **predicci√≥n de riesgo de cr√©dito**, siguiendo todas las fases del ciclo de modelado.  
Las conclusiones clave por etapa son:

### üîç 1. EDA y selecci√≥n de variables
- Se identific√≥ un **fuerte desbalance de clases**, lo que justific√≥ el uso de m√©tricas como AUC y KS en lugar de accuracy.  
- Varias variables financieras mostraron **alta dispersi√≥n y outliers**, lo que llev√≥ a utilizar imputaci√≥n robusta mediante mediana.  
- Se detectaron **correlaciones elevadas** entre indicadores de historial crediticio, reduciendo variables redundantes para evitar inflaci√≥n de informaci√≥n.  
- Algunas categ√≥ricas ten√≠an **cardinalidad excesiva**, lo que confirm√≥ la necesidad de filtrado previo y `OneHotEncoder(handle_unknown="ignore")`.

### ‚öôÔ∏è 2. Preprocesamiento
- El pipeline v3 permiti√≥ transformar el dataset enriquecido en una matriz estable de **357 features**, aplicando exactamente las mismas transformaciones en train y test.  
- Este dise√±o elimin√≥ riesgos de **fuga de informaci√≥n** y garantiz√≥ consistencia total en el scoring final.

### ü§ñ 3. Modelado y validaci√≥n
- LightGBM fue el modelo seleccionado tras comparar varias alternativas (Logistic Regression, XGBoost, CatBoost).  
- Las m√©tricas obtenidas (**AUC ‚âà 0.78**, **KS ‚âà 0.42**) demuestran un **modelo s√≥lido de scoring bancario**, con buena separaci√≥n entre clientes de alto y bajo riesgo.  
- La diferencia moderada entre train y valid confirma **ausencia de sobreajuste** y buena capacidad de generalizaci√≥n.

### üß† 4. Interpretabilidad
- SHAP permiti√≥ identificar los factores m√°s relevantes del riesgo, destacando:
  - historial de morosidad,  
  - variables del bur√≥,  
  - ratio de deuda/ingresos.  
- Esto aporta **transparencia y trazabilidad**, esenciales en entornos regulados.

### üì¶ 5. Implementaci√≥n y reproducibilidad
- El uso de artefactos (`pipeline`, `feature_names`, `modelo`) hace que el proyecto sea **100% replicable**.  
- El archivo de scoring final puede integrarse directamente en un sistema de decisi√≥n de cr√©dito.

En conjunto, se obtuvo un **modelo robusto, interpretable y listo para integraci√≥n**, demostrando un flujo profesional completo de Data Science aplicado al riesgo de cr√©dito.


## üë§ Sobre m√≠

Soy un profesional en formaci√≥n con enfoque en **Data Analytics** y en transici√≥n hacia **Data Science**, interesado en aplicar el an√°lisis de datos y el machine learning para resolver problemas reales de negocio.

Durante mi aprendizaje he trabajado especialmente con:

- **Python** para an√°lisis, modelado y visualizaci√≥n.  
- T√©cnicas de **preprocesamiento y preparaci√≥n de datos**.  
- Modelos supervisados aplicados a casos reales (como el **scoring de cr√©dito**).  
- **Interpretabilidad de modelos** mediante SHAP y an√°lisis de variables.  
- M√©tricas orientadas a negocio y validaci√≥n de modelos.

Me motiva construir soluciones basadas en datos que aporten valor, combinen rigor anal√≠tico y sean aplicables en entornos reales.

Actualmente busco **mi primera oportunidad profesional** como **Data Analyst** o **Data Scientist Junior**, y estoy abierto a colaborar en proyectos donde pueda seguir aprendiendo y aportando valor.

